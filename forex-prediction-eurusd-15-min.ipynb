{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Forex Trading Prediction - EURUSD Pair (4H):\n\nPerforming Forex Market Prediction for the EUR/USD pair (4H timeframe data) using Random Forest, SVM, and LSTM machine learning models and implementing cross-validation techniques to validate the results.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split, GridSearchCV, TimeSeriesSplit\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import mean_squared_error, accuracy_score\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam, RMSprop\nfrom tensorflow.keras.metrics import BinaryAccuracy\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","metadata":{"execution":{"iopub.status.busy":"2023-03-20T06:39:03.083302Z","iopub.execute_input":"2023-03-20T06:39:03.083750Z","iopub.status.idle":"2023-03-20T06:39:17.178841Z","shell.execute_reply.started":"2023-03-20T06:39:03.083701Z","shell.execute_reply":"2023-03-20T06:39:17.176965Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# importing data\ndata = pd.read_csv(\"/kaggle/input/eurusd-15-min/EURUSD_15m.csv\")\n","metadata":{"execution":{"iopub.status.busy":"2023-03-20T06:39:17.181796Z","iopub.execute_input":"2023-03-20T06:39:17.183264Z","iopub.status.idle":"2023-03-20T06:39:17.729101Z","shell.execute_reply.started":"2023-03-20T06:39:17.183197Z","shell.execute_reply":"2023-03-20T06:39:17.727514Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"data.columns=['Date', 'Open', 'High', 'Low', 'Close', 'Volume']\n\n# converting the date column to a datetime object and setting it as the index\ndata['Date'] = pd.to_datetime(data['Date'])\ndata.set_index('Date', inplace=True) \navg_vol = data.loc[data['Volume'].ne(0) & data['Volume'].notna(), 'Volume'].mean()\n\n# Replace 0 and null values with the calculated average\ndata['Volume'] = data['Volume'].replace(0, avg_vol).fillna(avg_vol)\ndata.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2023-03-20T06:39:17.731175Z","iopub.execute_input":"2023-03-20T06:39:17.731666Z","iopub.status.idle":"2023-03-20T06:39:43.337709Z","shell.execute_reply.started":"2023-03-20T06:39:17.731629Z","shell.execute_reply":"2023-03-20T06:39:43.336228Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"Open      0\nHigh      0\nLow       0\nClose     0\nVolume    0\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2023-03-20T06:39:43.340612Z","iopub.execute_input":"2023-03-20T06:39:43.341017Z","iopub.status.idle":"2023-03-20T06:39:43.368070Z","shell.execute_reply.started":"2023-03-20T06:39:43.340978Z","shell.execute_reply":"2023-03-20T06:39:43.366450Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                        Open     High      Low    Close       Volume\nDate                                                                \n2010-01-01 00:00:00  1.43283  1.43293  1.43224  1.43293  608600007.0\n2010-01-01 00:15:00  1.43285  1.43295  1.43229  1.43275  535600003.0\n2010-01-01 00:30:00  1.43280  1.43303  1.43239  1.43281  436299999.0\n2010-01-01 00:45:00  1.43285  1.43294  1.43229  1.43276  614299997.0\n2010-01-01 01:00:00  1.43287  1.43292  1.43206  1.43282  705300009.0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Open</th>\n      <th>High</th>\n      <th>Low</th>\n      <th>Close</th>\n      <th>Volume</th>\n    </tr>\n    <tr>\n      <th>Date</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2010-01-01 00:00:00</th>\n      <td>1.43283</td>\n      <td>1.43293</td>\n      <td>1.43224</td>\n      <td>1.43293</td>\n      <td>608600007.0</td>\n    </tr>\n    <tr>\n      <th>2010-01-01 00:15:00</th>\n      <td>1.43285</td>\n      <td>1.43295</td>\n      <td>1.43229</td>\n      <td>1.43275</td>\n      <td>535600003.0</td>\n    </tr>\n    <tr>\n      <th>2010-01-01 00:30:00</th>\n      <td>1.43280</td>\n      <td>1.43303</td>\n      <td>1.43239</td>\n      <td>1.43281</td>\n      <td>436299999.0</td>\n    </tr>\n    <tr>\n      <th>2010-01-01 00:45:00</th>\n      <td>1.43285</td>\n      <td>1.43294</td>\n      <td>1.43229</td>\n      <td>1.43276</td>\n      <td>614299997.0</td>\n    </tr>\n    <tr>\n      <th>2010-01-01 01:00:00</th>\n      <td>1.43287</td>\n      <td>1.43292</td>\n      <td>1.43206</td>\n      <td>1.43282</td>\n      <td>705300009.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"data.tail()","metadata":{"execution":{"iopub.status.busy":"2023-03-20T06:39:43.370147Z","iopub.execute_input":"2023-03-20T06:39:43.370543Z","iopub.status.idle":"2023-03-20T06:39:43.388475Z","shell.execute_reply.started":"2023-03-20T06:39:43.370509Z","shell.execute_reply":"2023-03-20T06:39:43.387202Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                       Open    High     Low   Close        Volume\nDate                                                             \n2016-12-31 22:45:00  1.0515  1.0515  1.0515  1.0515  2.281625e+09\n2016-12-31 23:00:00  1.0515  1.0515  1.0515  1.0515  2.281625e+09\n2016-12-31 23:15:00  1.0515  1.0515  1.0515  1.0515  2.281625e+09\n2016-12-31 23:30:00  1.0515  1.0515  1.0515  1.0515  2.281625e+09\n2016-12-31 23:45:00  1.0515  1.0515  1.0515  1.0515  2.281625e+09","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Open</th>\n      <th>High</th>\n      <th>Low</th>\n      <th>Close</th>\n      <th>Volume</th>\n    </tr>\n    <tr>\n      <th>Date</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2016-12-31 22:45:00</th>\n      <td>1.0515</td>\n      <td>1.0515</td>\n      <td>1.0515</td>\n      <td>1.0515</td>\n      <td>2.281625e+09</td>\n    </tr>\n    <tr>\n      <th>2016-12-31 23:00:00</th>\n      <td>1.0515</td>\n      <td>1.0515</td>\n      <td>1.0515</td>\n      <td>1.0515</td>\n      <td>2.281625e+09</td>\n    </tr>\n    <tr>\n      <th>2016-12-31 23:15:00</th>\n      <td>1.0515</td>\n      <td>1.0515</td>\n      <td>1.0515</td>\n      <td>1.0515</td>\n      <td>2.281625e+09</td>\n    </tr>\n    <tr>\n      <th>2016-12-31 23:30:00</th>\n      <td>1.0515</td>\n      <td>1.0515</td>\n      <td>1.0515</td>\n      <td>1.0515</td>\n      <td>2.281625e+09</td>\n    </tr>\n    <tr>\n      <th>2016-12-31 23:45:00</th>\n      <td>1.0515</td>\n      <td>1.0515</td>\n      <td>1.0515</td>\n      <td>1.0515</td>\n      <td>2.281625e+09</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"!pip install pandas_ta # technical analysis library","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-03-20T06:39:43.392328Z","iopub.execute_input":"2023-03-20T06:39:43.392926Z","iopub.status.idle":"2023-03-20T06:39:58.771364Z","shell.execute_reply.started":"2023-03-20T06:39:43.392883Z","shell.execute_reply":"2023-03-20T06:39:58.769735Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Collecting pandas_ta\n  Downloading pandas_ta-0.3.14b.tar.gz (115 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.1/115.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from pandas_ta) (1.3.5)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->pandas_ta) (2022.7.1)\nRequirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.7/site-packages (from pandas->pandas_ta) (1.21.6)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->pandas_ta) (2.8.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->pandas_ta) (1.16.0)\nBuilding wheels for collected packages: pandas_ta\n  Building wheel for pandas_ta (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pandas_ta: filename=pandas_ta-0.3.14b0-py3-none-any.whl size=218923 sha256=ef1c8cd3129d31057c563d04a8100273fa3dc428868a943942f7721b2528ffc8\n  Stored in directory: /root/.cache/pip/wheels/7e/c3/40/fb36bba6c91caf81c39791388c71baca9635cbefd8e3bd48a7\nSuccessfully built pandas_ta\nInstalling collected packages: pandas_ta\nSuccessfully installed pandas_ta-0.3.14b0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import pandas_ta as ta # technical analysis package\n\ndata['RSI'] = data.ta.rsi(length = 14) \ndata.head(15)","metadata":{"execution":{"iopub.status.busy":"2023-03-20T06:39:58.776396Z","iopub.execute_input":"2023-03-20T06:39:58.776817Z","iopub.status.idle":"2023-03-20T06:39:59.371325Z","shell.execute_reply.started":"2023-03-20T06:39:58.776764Z","shell.execute_reply":"2023-03-20T06:39:59.369906Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                        Open     High      Low    Close       Volume  \\\nDate                                                                   \n2010-01-01 00:00:00  1.43283  1.43293  1.43224  1.43293  608600007.0   \n2010-01-01 00:15:00  1.43285  1.43295  1.43229  1.43275  535600003.0   \n2010-01-01 00:30:00  1.43280  1.43303  1.43239  1.43281  436299999.0   \n2010-01-01 00:45:00  1.43285  1.43294  1.43229  1.43276  614299997.0   \n2010-01-01 01:00:00  1.43287  1.43292  1.43206  1.43282  705300009.0   \n2010-01-01 01:15:00  1.43290  1.43299  1.43212  1.43292  427300006.0   \n2010-01-01 01:30:00  1.43267  1.43305  1.43209  1.43282  399200001.0   \n2010-01-01 01:45:00  1.43280  1.43302  1.43218  1.43249  481200008.0   \n2010-01-01 02:00:00  1.43279  1.43303  1.43237  1.43301  607599997.0   \n2010-01-01 02:15:00  1.43301  1.43303  1.43222  1.43271  469600000.0   \n2010-01-01 02:30:00  1.43285  1.43305  1.43228  1.43285  492100001.0   \n2010-01-01 02:45:00  1.43288  1.43302  1.43218  1.43278  940599988.0   \n2010-01-01 03:00:00  1.43285  1.43302  1.43222  1.43287  931900000.0   \n2010-01-01 03:15:00  1.43280  1.43306  1.43219  1.43292  729700007.0   \n2010-01-01 03:30:00  1.43280  1.43302  1.43219  1.43276  898299997.0   \n\n                           RSI  \nDate                            \n2010-01-01 00:00:00        NaN  \n2010-01-01 00:15:00        NaN  \n2010-01-01 00:30:00        NaN  \n2010-01-01 00:45:00        NaN  \n2010-01-01 01:00:00        NaN  \n2010-01-01 01:15:00        NaN  \n2010-01-01 01:30:00        NaN  \n2010-01-01 01:45:00        NaN  \n2010-01-01 02:00:00        NaN  \n2010-01-01 02:15:00        NaN  \n2010-01-01 02:30:00        NaN  \n2010-01-01 02:45:00        NaN  \n2010-01-01 03:00:00        NaN  \n2010-01-01 03:15:00        NaN  \n2010-01-01 03:30:00  46.520325  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Open</th>\n      <th>High</th>\n      <th>Low</th>\n      <th>Close</th>\n      <th>Volume</th>\n      <th>RSI</th>\n    </tr>\n    <tr>\n      <th>Date</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2010-01-01 00:00:00</th>\n      <td>1.43283</td>\n      <td>1.43293</td>\n      <td>1.43224</td>\n      <td>1.43293</td>\n      <td>608600007.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2010-01-01 00:15:00</th>\n      <td>1.43285</td>\n      <td>1.43295</td>\n      <td>1.43229</td>\n      <td>1.43275</td>\n      <td>535600003.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2010-01-01 00:30:00</th>\n      <td>1.43280</td>\n      <td>1.43303</td>\n      <td>1.43239</td>\n      <td>1.43281</td>\n      <td>436299999.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2010-01-01 00:45:00</th>\n      <td>1.43285</td>\n      <td>1.43294</td>\n      <td>1.43229</td>\n      <td>1.43276</td>\n      <td>614299997.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2010-01-01 01:00:00</th>\n      <td>1.43287</td>\n      <td>1.43292</td>\n      <td>1.43206</td>\n      <td>1.43282</td>\n      <td>705300009.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2010-01-01 01:15:00</th>\n      <td>1.43290</td>\n      <td>1.43299</td>\n      <td>1.43212</td>\n      <td>1.43292</td>\n      <td>427300006.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2010-01-01 01:30:00</th>\n      <td>1.43267</td>\n      <td>1.43305</td>\n      <td>1.43209</td>\n      <td>1.43282</td>\n      <td>399200001.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2010-01-01 01:45:00</th>\n      <td>1.43280</td>\n      <td>1.43302</td>\n      <td>1.43218</td>\n      <td>1.43249</td>\n      <td>481200008.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2010-01-01 02:00:00</th>\n      <td>1.43279</td>\n      <td>1.43303</td>\n      <td>1.43237</td>\n      <td>1.43301</td>\n      <td>607599997.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2010-01-01 02:15:00</th>\n      <td>1.43301</td>\n      <td>1.43303</td>\n      <td>1.43222</td>\n      <td>1.43271</td>\n      <td>469600000.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2010-01-01 02:30:00</th>\n      <td>1.43285</td>\n      <td>1.43305</td>\n      <td>1.43228</td>\n      <td>1.43285</td>\n      <td>492100001.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2010-01-01 02:45:00</th>\n      <td>1.43288</td>\n      <td>1.43302</td>\n      <td>1.43218</td>\n      <td>1.43278</td>\n      <td>940599988.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2010-01-01 03:00:00</th>\n      <td>1.43285</td>\n      <td>1.43302</td>\n      <td>1.43222</td>\n      <td>1.43287</td>\n      <td>931900000.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2010-01-01 03:15:00</th>\n      <td>1.43280</td>\n      <td>1.43306</td>\n      <td>1.43219</td>\n      <td>1.43292</td>\n      <td>729700007.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2010-01-01 03:30:00</th>\n      <td>1.43280</td>\n      <td>1.43302</td>\n      <td>1.43219</td>\n      <td>1.43276</td>\n      <td>898299997.0</td>\n      <td>46.520325</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"data = data.dropna()\ndata.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2023-03-20T06:39:59.372801Z","iopub.execute_input":"2023-03-20T06:39:59.374988Z","iopub.status.idle":"2023-03-20T06:39:59.423002Z","shell.execute_reply.started":"2023-03-20T06:39:59.374946Z","shell.execute_reply":"2023-03-20T06:39:59.421616Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"Open      0\nHigh      0\nLow       0\nClose     0\nVolume    0\nRSI       0\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"# Create a new column that represents the difference between the opening and closing prices of the currency pair\ndata['Target'] = data.apply(lambda row: 1 if row['Close'] - row['Open'] > 0 else 0, axis=1)\ndata.tail()","metadata":{"execution":{"iopub.status.busy":"2023-03-20T06:39:59.425093Z","iopub.execute_input":"2023-03-20T06:39:59.425472Z","iopub.status.idle":"2023-03-20T06:40:03.872748Z","shell.execute_reply.started":"2023-03-20T06:39:59.425435Z","shell.execute_reply":"2023-03-20T06:40:03.871319Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"                       Open    High     Low   Close        Volume        RSI  \\\nDate                                                                           \n2016-12-31 22:45:00  1.0515  1.0515  1.0515  1.0515  2.281625e+09  37.429023   \n2016-12-31 23:00:00  1.0515  1.0515  1.0515  1.0515  2.281625e+09  37.429023   \n2016-12-31 23:15:00  1.0515  1.0515  1.0515  1.0515  2.281625e+09  37.429023   \n2016-12-31 23:30:00  1.0515  1.0515  1.0515  1.0515  2.281625e+09  37.429023   \n2016-12-31 23:45:00  1.0515  1.0515  1.0515  1.0515  2.281625e+09  37.429023   \n\n                     Target  \nDate                         \n2016-12-31 22:45:00       0  \n2016-12-31 23:00:00       0  \n2016-12-31 23:15:00       0  \n2016-12-31 23:30:00       0  \n2016-12-31 23:45:00       0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Open</th>\n      <th>High</th>\n      <th>Low</th>\n      <th>Close</th>\n      <th>Volume</th>\n      <th>RSI</th>\n      <th>Target</th>\n    </tr>\n    <tr>\n      <th>Date</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2016-12-31 22:45:00</th>\n      <td>1.0515</td>\n      <td>1.0515</td>\n      <td>1.0515</td>\n      <td>1.0515</td>\n      <td>2.281625e+09</td>\n      <td>37.429023</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2016-12-31 23:00:00</th>\n      <td>1.0515</td>\n      <td>1.0515</td>\n      <td>1.0515</td>\n      <td>1.0515</td>\n      <td>2.281625e+09</td>\n      <td>37.429023</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2016-12-31 23:15:00</th>\n      <td>1.0515</td>\n      <td>1.0515</td>\n      <td>1.0515</td>\n      <td>1.0515</td>\n      <td>2.281625e+09</td>\n      <td>37.429023</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2016-12-31 23:30:00</th>\n      <td>1.0515</td>\n      <td>1.0515</td>\n      <td>1.0515</td>\n      <td>1.0515</td>\n      <td>2.281625e+09</td>\n      <td>37.429023</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2016-12-31 23:45:00</th>\n      <td>1.0515</td>\n      <td>1.0515</td>\n      <td>1.0515</td>\n      <td>1.0515</td>\n      <td>2.281625e+09</td>\n      <td>37.429023</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"data = data.drop_duplicates()","metadata":{"execution":{"iopub.status.busy":"2023-03-20T06:40:03.877957Z","iopub.execute_input":"2023-03-20T06:40:03.878707Z","iopub.status.idle":"2023-03-20T06:40:04.021756Z","shell.execute_reply.started":"2023-03-20T06:40:03.878660Z","shell.execute_reply":"2023-03-20T06:40:04.020228Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Create the input features (X) and target variable (y).\nX = data.drop(['Target'], axis=1)\ny = data['Target']","metadata":{"execution":{"iopub.status.busy":"2023-03-20T06:40:04.023289Z","iopub.execute_input":"2023-03-20T06:40:04.024184Z","iopub.status.idle":"2023-03-20T06:40:04.038340Z","shell.execute_reply.started":"2023-03-20T06:40:04.024138Z","shell.execute_reply":"2023-03-20T06:40:04.036882Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Perform feature scaling using MinMaxScaler.\nscaler = MinMaxScaler()\nX_scaled = scaler.fit_transform(X)","metadata":{"execution":{"iopub.status.busy":"2023-03-20T06:40:04.040638Z","iopub.execute_input":"2023-03-20T06:40:04.041200Z","iopub.status.idle":"2023-03-20T06:40:04.071968Z","shell.execute_reply.started":"2023-03-20T06:40:04.041142Z","shell.execute_reply":"2023-03-20T06:40:04.070565Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Split the data into training and testing sets.\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25, shuffle=False)\n","metadata":{"execution":{"iopub.status.busy":"2023-03-20T06:40:04.073201Z","iopub.execute_input":"2023-03-20T06:40:04.073877Z","iopub.status.idle":"2023-03-20T06:40:04.092627Z","shell.execute_reply.started":"2023-03-20T06:40:04.073835Z","shell.execute_reply":"2023-03-20T06:40:04.091502Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Let's build and train the Random Forest model\nrf_model = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_model.fit(X_train, y_train)\nrf_pred = rf_model.predict(X_test)\nrf_train_pred = rf_model.predict(X_train)\n\ntrain_accuracy = accuracy_score(y_train, rf_train_pred)\ntest_accuracy = accuracy_score(y_test, rf_pred)\n\nprint(f'RF Train Accuracy: {train_accuracy}')\nprint(f'RF Test Accuracy: {test_accuracy}')\n\n# Check for underfitting or overfitting\nif train_accuracy < test_accuracy:\n    if abs(train_accuracy - test_accuracy) > 0.1 * train_accuracy:\n        print(\"The model might be overfitting.\")\n    else:\n        print(\"The model seems to be performing well.\")\nelif train_accuracy > test_accuracy:\n    if abs(train_accuracy - test_accuracy) > 0.1 * test_accuracy:\n        print(\"The model might be underfitting.\")\n    else:\n        print(\"The model seems to be performing well.\")\nelse:\n    print(\"The model seems to be performing well.\")\n\n","metadata":{"_kg_hide-output":false,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-03-20T06:40:04.093859Z","iopub.execute_input":"2023-03-20T06:40:04.096498Z","iopub.status.idle":"2023-03-20T06:41:42.107306Z","shell.execute_reply.started":"2023-03-20T06:40:04.096434Z","shell.execute_reply":"2023-03-20T06:41:42.105831Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"RF Train Accuracy: 1.0\nRF Test Accuracy: 0.5967503361721201\nThe model might be underfitting.\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\n\n# SVM\nsvm_model = SVC(kernel='rbf', C=1e3, gamma=0.1)\nsvm_model.fit(X_train, y_train)\nsvm_pred = svm_model.predict(X_test)\nsvm_train_pred = svm_model.predict(X_train)\n\ntrain_accuracy = accuracy_score(y_train, svm_train_pred)\ntest_accuracy = accuracy_score(y_test, svm_pred)\n\nprint(f'SVM Train Accuracy: {train_accuracy}')\nprint(f'SVM Test Accuracy: {test_accuracy}')\n\n# Check for underfitting or overfitting\nif train_accuracy < test_accuracy:\n    if abs(train_accuracy - test_accuracy) > 0.1 * train_accuracy:\n        print(\"The model might be overfitting.\")\n    else:\n        print(\"The model seems to be performing well.\")\nelif train_accuracy > test_accuracy:\n    if abs(train_accuracy - test_accuracy) > 0.1 * test_accuracy:\n        print(\"The model might be underfitting.\")\n    else:\n        print(\"The model seems to be performing well.\")\nelse:\n    print(\"The model seems to be performing well.\")","metadata":{"execution":{"iopub.status.busy":"2023-03-20T06:41:42.108742Z","iopub.execute_input":"2023-03-20T06:41:42.109235Z","iopub.status.idle":"2023-03-20T07:01:42.819737Z","shell.execute_reply.started":"2023-03-20T06:41:42.109179Z","shell.execute_reply":"2023-03-20T07:01:42.818103Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"SVM Train Accuracy: 0.9562446771952368\nSVM Test Accuracy: 0.9316001792917974\nThe model seems to be performing well.\n","output_type":"stream"}]},{"cell_type":"code","source":"from tensorflow.keras.metrics import BinaryAccuracy\n\n# Let's create and train the LSTM model.\n# Reshape the input data for the LSTM model\nX_train_lstm = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\nX_test_lstm = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n\n# LSTM\nlstm_model = Sequential()\nlstm_model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train_lstm.shape[1], X_train_lstm.shape[2])))\nlstm_model.add(Dropout(0.2))\nlstm_model.add(LSTM(units=50))\nlstm_model.add(Dropout(0.2))\nlstm_model.add(Dense(1, activation='sigmoid'))\n\nlstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[BinaryAccuracy()])\nlstm_model.fit(X_train_lstm, y_train, epochs=50, batch_size=32)\nlstm_pred = np.round(lstm_model.predict(X_test_lstm))\nlstm_train_pred = np.round(lstm_model.predict(X_train_lstm))\n\ntrain_accuracy = accuracy_score(y_train, lstm_train_pred)\ntest_accuracy = accuracy_score(y_test, lstm_pred)\n\nprint(f'LSTM Train Accuracy: {train_accuracy}')\nprint(f'LSTM Test Accuracy: {test_accuracy}')\n\n# Check for underfitting or overfitting\nif train_accuracy < test_accuracy:\n    if abs(train_accuracy - test_accuracy) > 0.1 * train_accuracy:\n        print(\"The model might be overfitting.\")\n    else:\n        print(\"The model seems to be performing well.\")\nelif train_accuracy > test_accuracy:\n    if abs(train_accuracy - test_accuracy) > 0.1 * test_accuracy:\n        print(\"The model might be underfitting.\")\n    else:\n        print(\"The model seems to be performing well.\")\nelse:\n    print(\"The model seems to be performing well.\")\n","metadata":{"execution":{"iopub.status.busy":"2023-03-20T07:01:42.821966Z","iopub.execute_input":"2023-03-20T07:01:42.822326Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/50\n4184/4184 [==============================] - 27s 5ms/step - loss: 0.6714 - binary_accuracy: 0.5818\nEpoch 2/50\n4184/4184 [==============================] - 23s 6ms/step - loss: 0.6685 - binary_accuracy: 0.5878\nEpoch 3/50\n4184/4184 [==============================] - 23s 6ms/step - loss: 0.6675 - binary_accuracy: 0.5890\nEpoch 4/50\n4184/4184 [==============================] - 23s 6ms/step - loss: 0.6673 - binary_accuracy: 0.5896\nEpoch 5/50\n4184/4184 [==============================] - 23s 5ms/step - loss: 0.6671 - binary_accuracy: 0.5909\nEpoch 6/50\n4184/4184 [==============================] - 24s 6ms/step - loss: 0.6670 - binary_accuracy: 0.5910\nEpoch 7/50\n3221/4184 [======================>.......] - ETA: 5s - loss: 0.6667 - binary_accuracy: 0.5900","output_type":"stream"},{"text":"IOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n","name":"stderr","output_type":"stream"},{"name":"stdout","text":"4184/4184 [==============================] - 23s 5ms/step - loss: 0.6664 - binary_accuracy: 0.5918\nEpoch 12/50\n4184/4184 [==============================] - 23s 5ms/step - loss: 0.6663 - binary_accuracy: 0.5907\nEpoch 13/50\n4184/4184 [==============================] - 23s 5ms/step - loss: 0.6661 - binary_accuracy: 0.5913\nEpoch 14/50\n4184/4184 [==============================] - 23s 5ms/step - loss: 0.6658 - binary_accuracy: 0.5921\nEpoch 15/50\n4184/4184 [==============================] - 23s 5ms/step - loss: 0.6658 - binary_accuracy: 0.5928\nEpoch 16/50\n4184/4184 [==============================] - 23s 6ms/step - loss: 0.6657 - binary_accuracy: 0.5934\nEpoch 17/50\n4184/4184 [==============================] - 23s 6ms/step - loss: 0.6657 - binary_accuracy: 0.5917\nEpoch 18/50\n4184/4184 [==============================] - 23s 6ms/step - loss: 0.6655 - binary_accuracy: 0.5935\nEpoch 19/50\n4184/4184 [==============================] - 23s 6ms/step - loss: 0.6654 - binary_accuracy: 0.5929\nEpoch 20/50\n4184/4184 [==============================] - 24s 6ms/step - loss: 0.6655 - binary_accuracy: 0.5929\nEpoch 21/50\n4184/4184 [==============================] - 23s 5ms/step - loss: 0.6652 - binary_accuracy: 0.5931\nEpoch 22/50\n4184/4184 [==============================] - 23s 6ms/step - loss: 0.6653 - binary_accuracy: 0.5935\nEpoch 23/50\n4184/4184 [==============================] - 23s 5ms/step - loss: 0.6650 - binary_accuracy: 0.5932\nEpoch 24/50\n4184/4184 [==============================] - 23s 5ms/step - loss: 0.6649 - binary_accuracy: 0.5941\nEpoch 25/50\n4184/4184 [==============================] - 23s 5ms/step - loss: 0.6647 - binary_accuracy: 0.5938\nEpoch 26/50\n4184/4184 [==============================] - 23s 5ms/step - loss: 0.6647 - binary_accuracy: 0.5952\nEpoch 27/50\n4184/4184 [==============================] - 23s 5ms/step - loss: 0.6646 - binary_accuracy: 0.5936\nEpoch 28/50\n4184/4184 [==============================] - 23s 5ms/step - loss: 0.6644 - binary_accuracy: 0.5940\nEpoch 29/50\n4184/4184 [==============================] - 22s 5ms/step - loss: 0.6643 - binary_accuracy: 0.5950\nEpoch 30/50\n4184/4184 [==============================] - 23s 5ms/step - loss: 0.6642 - binary_accuracy: 0.5950\nEpoch 31/50\n4184/4184 [==============================] - 23s 5ms/step - loss: 0.6641 - binary_accuracy: 0.5946\nEpoch 32/50\n4184/4184 [==============================] - 22s 5ms/step - loss: 0.6640 - binary_accuracy: 0.5953\nEpoch 33/50\n4184/4184 [==============================] - 23s 5ms/step - loss: 0.6640 - binary_accuracy: 0.5949\nEpoch 34/50\n3369/4184 [=======================>......] - ETA: 4s - loss: 0.6636 - binary_accuracy: 0.5951","output_type":"stream"}]},{"cell_type":"code","source":"# Evaluate the performance of each model using accuracy.\nrf_accuracy = accuracy_score(y_test, rf_pred)\nsvm_accuracy = accuracy_score(y_test, svm_pred)\nlstm_accuracy = accuracy_score(y_test, lstm_pred)\nprint(f'Random Forest Accuracy: {rf_accuracy}')\nprint(f'SVM Accuracy: {svm_accuracy}')\nprint(f'LSTM Accuracy: {lstm_accuracy}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Validating the results using  cross-validation techniques.\nTo validate the results using cross-validation techniques, we can use TimeSeriesSplit from scikit-learn. TimeSeriesSplit is a variation of k-fold cross-validation specifically designed for time series data. Let's update the code for the Random Forest and SVM models with cross-validation.","metadata":{}},{"cell_type":"markdown","source":"# Performing Gridsearch CV on the 3 models","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import make_scorer\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom sklearn.model_selection import TimeSeriesSplit, cross_val_score\n\ntscv = TimeSeriesSplit(n_splits=5)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"def rmse(y_true, y_pred):\n    return np.sqrt(mean_squared_error(y_true, y_pred))\n\nrmse_scorer = make_scorer(rmse, greater_is_better=False)\nscoring_methods = {'MSE': 'neg_mean_squared_error', 'RMSE': rmse_scorer, 'MAE': 'neg_mean_absolute_error', 'R-squared': 'r2'}\n","metadata":{}},{"cell_type":"code","source":"# RF with GS CV\ndef calc_accuracy(y_true, y_pred):\n    return accuracy_score(y_true, np.round(y_pred))\n\naccuracy_scorer = make_scorer(calc_accuracy, greater_is_better=True)\n\n# Random Forest with GridSearchCV\nrf_param_grid = {\n    'n_estimators': [200],\n    'max_depth': [None],\n    'min_samples_split': [2]\n}\n\nrf_grid = GridSearchCV(RandomForestClassifier(random_state=42), rf_param_grid, cv=tscv, scoring=accuracy_scorer, n_jobs=-1)\nrf_grid.fit(X_train, y_train)\nrf_best = rf_grid.best_params_\nrf_best_accuracy = rf_grid.best_score_\nprint(f'RF Best Parms: {rf_best}')\n\n# Random Forest with best parameters\nrf_train_pred = rf_grid.predict(X_train)\nrf_test_pred = rf_grid.predict(X_test)\nrf_train_accuracy = calc_accuracy(y_train, rf_train_pred)\nrf_test_accuracy = calc_accuracy(y_test, rf_test_pred)\nprint(f'RF Train Accuracy: {rf_train_accuracy}')\nprint(f'RF Test Accuracy: {rf_test_accuracy}')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# SVM with GridSearchCV\nsvm_param_grid = {\n    'C': [1e1, 1e2, 1e3],\n    'gamma': [0.01, 0.1, 1],\n    'kernel': ['rbf']\n}\n# SVM Best Parms: {'C': 1000.0, 'gamma': 0.1, 'kernel': 'rbf'}\nsvm_grid = GridSearchCV(SVC(), svm_param_grid, cv=tscv, scoring=accuracy_scorer, n_jobs=-1)\nsvm_grid.fit(X_train, y_train)\nsvm_best = svm_grid.best_params_\nsvm_best_accuracy = svm_grid.best_score_\nprint(f'SVM Best Parms: {svm_best}')\n\n# SVM with best parameters\nsvm_train_pred = svm_grid.predict(X_train)\nsvm_test_pred = svm_grid.predict(X_test)\nsvm_train_accuracy = calc_accuracy(y_train, svm_train_pred)\nsvm_test_accuracy = calc_accuracy(y_test, svm_test_pred)\nprint(f'SVM Train Accuracy: {svm_train_accuracy}')\nprint(f'SVM Test Accuracy: {svm_test_accuracy}')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For the LSTM model, since Keras doesn't have direct support for GridSearchCV, we need to create a custom wrapper and use KerasRegressor from keras.wrappers.scikit_learn. Let's update the code for the LSTM model with GridSearchCV.","metadata":{}},{"cell_type":"code","source":"from keras.wrappers.scikit_learn import KerasClassifier\n\n# Reshape the input data for the LSTM model\nX_train_lstm = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\nX_test_lstm = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n\ndef create_lstm_model(units=50, dropout_rate=0.2, optimizer='adam'):\n    model = Sequential()\n    model.add(LSTM(units=units, return_sequences=True, input_shape=(1, X_train.shape[1])))\n    model.add(Dropout(dropout_rate))\n    model.add(LSTM(units=units))\n    model.add(Dropout(dropout_rate))\n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n    return model\n\nlstm_model_grid = KerasClassifier(build_fn=create_lstm_model)\n\nlstm_param_grid = {\n    'units': [50],\n    'dropout_rate': [0.2],\n    'optimizer': ['adam'],\n    'epochs': [50],\n    'batch_size': [32]\n}\n\nlstm_grid = GridSearchCV(lstm_model_grid, lstm_param_grid, cv=tscv, scoring='accuracy', n_jobs=-1)\nlstm_grid.fit(X_train_lstm, y_train)\nlstm_best = lstm_grid.best_params_\nlstm_best_accuracy = lstm_grid.best_score_\nprint(f'LSTM Best Params: {lstm_best}')\n\n# LSTM with best parameters\nlstm_train_pred = (lstm_grid.predict(X_train_lstm) > 0.5).astype(int)\nlstm_test_pred = (lstm_grid.predict(X_test_lstm) > 0.5).astype(int)\nlstm_train_accuracy = calc_accuracy(y_train, lstm_train_pred)\nlstm_test_accuracy = calc_accuracy(y_test, lstm_test_pred)\nprint(f'LSTM Train Accuracy: {lstm_train_accuracy}')\nprint(f'LSTM Test Accuracy: {lstm_test_accuracy}')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"def get_best_metric(cv_results, scoring_methods):\n    best_scores = {}\n    for metric in scoring_methods:\n        if metric == 'R-squared':\n            best_scores[metric] = cv_results[f'mean_test_{metric}'].max()\n        else:\n            best_scores[metric] = -cv_results[f'mean_test_{metric}'].min()\n    best_metric = max(best_scores, key=best_scores.get)\n    return best_metric\n\n# Get the best metric for each model\nrf_best_metric = get_best_metric(rf_grid.cv_results_, scoring_methods)\nsvm_best_metric = get_best_metric(svm_grid.cv_results_, scoring_methods)\nlstm_best_metric = get_best_metric(lstm_grid.cv_results_, scoring_methods)\n\nprint(\"Best metric for Random Forest:\", rf_best_metric)\nprint(\"Best metric for SVM:\", svm_best_metric)\nprint(\"Best metric for LSTM:\", lstm_best_metric)\n\ndef best_scoring_method(scores):\n    best_metric = max(scores, key=scores.get)\n    best_value = scores[best_metric]\n    return f\"{best_metric}: {best_value:.4f}\"","metadata":{"execution":{"iopub.status.busy":"2023-03-19T21:27:03.670121Z","iopub.status.idle":"2023-03-19T21:27:03.670746Z","shell.execute_reply.started":"2023-03-19T21:27:03.670455Z","shell.execute_reply":"2023-03-19T21:27:03.670495Z"}}},{"cell_type":"code","source":"def check_fit(train_accuracy, test_accuracy):\n    if train_accuracy < test_accuracy:\n        if abs(train_accuracy - test_accuracy) > 0.1 * train_accuracy:\n            return \"Overfitting\"\n        else:\n            return \"Good fit\"\n    elif train_accuracy > test_accuracy:\n        if abs(train_accuracy - test_accuracy) > 0.1 * test_accuracy:\n            return \"Underfitting\"\n        else:\n            return \"Good fit\"\n    else:\n        return \"Good fit\"\n\nrf_fit = check_fit(rf_train_accuracy, rf_test_accuracy)\nsvm_fit = check_fit(svm_train_accuracy, svm_test_accuracy)\nlstm_fit = check_fit(lstm_train_accuracy, lstm_test_accuracy)\n\nfinal_results = pd.DataFrame({\n    'Model': ['Random Forest', 'SVM', 'LSTM'],\n    'Best Parameters': [rf_best, svm_best, lstm_best],\n    'Train Accuracy': [rf_train_accuracy, svm_train_accuracy, lstm_train_accuracy],\n    'Test Accuracy': [rf_test_accuracy, svm_test_accuracy, lstm_test_accuracy],\n    'Fit': [rf_fit, svm_fit, lstm_fit],\n})\n\ndisplay(final_results)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import clone_model\n\ndef train_ensemble(X_train, y_train, X_test, y_test, num_models, units, dropout_rate, optimizer, epochs, batch_size):\n    models = []\n    preds = np.zeros((num_models, y_test.shape[0]))\n    best_accuracy = 0.0\n    best_model = None\n    best_params = None\n    \n    for i in range(num_models):\n        print(f\"Training model {i + 1}...\")\n        model = create_lstm_model(units=units, dropout_rate=dropout_rate, optimizer=optimizer)\n        model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n        models.append(model)\n\n        test_preds = model.predict(X_test)\n        preds[i, :] = test_preds[:, 0]\n\n        # Evaluate the model's accuracy\n        train_preds = model.predict(X_train)\n        train_preds_binary = threshold_predictions(train_preds, threshold=0.5)\n        train_accuracy = accuracy_score(y_train, train_preds_binary)\n        test_accuracy = accuracy_score(y_test, threshold_predictions(test_preds, threshold=0.5))\n\n        # Check if this model has the highest accuracy so far\n        if test_accuracy > best_accuracy:\n            best_accuracy = test_accuracy\n            best_model = model\n            best_params = {\n                'units': units,\n                'dropout_rate': dropout_rate,\n                'optimizer': optimizer,\n                'epochs': epochs,\n                'batch_size': batch_size\n            }\n\n        # Output this model's accuracy\n        print(f\"Model {i + 1} Train Accuracy: {train_accuracy:.4f}\")\n        print(f\"Model {i + 1} Test Accuracy: {test_accuracy:.4f}\")\n        \n    # Output the best model's hyperparameters and accuracy\n    print(f\"Best Model Hyperparameters: {best_params}\")\n    print(f\"Best Model Test Accuracy: {best_accuracy:.4f}\")\n    \n    # Average the predictions from all models\n    ensemble_preds = np.mean(preds, axis=0)\n    return models, ensemble_preds\n\ndef threshold_predictions(predictions, threshold=0.5):\n    return (predictions >= threshold).astype(int)\n\n# Scale the input features\nscaler = MinMaxScaler(feature_range=(0, 1))\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Reshape the data for LSTM input\nX_train_lstm = np.reshape(X_train_scaled, (X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\nX_test_lstm = np.reshape(X_test_scaled, (X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))\n\n# Use the build_lstm_model and train_ensemble functions provided in the previous response\n\n# Set the hyperparameters for the ensemble of LSTM models\nnum_models = 5\nunits = 50\ndropout_rate = 0.2\noptimizer = Adam()\nepochs = 50\nbatch_size = 32\n\n# Train the ensemble of LSTM models and obtain the averaged predictions\nensemble_models, ensemble_preds = train_ensemble(X_train_lstm, y_train, X_test_lstm, y_test, num_models, units, dropout_rate, optimizer, epochs, batch_size)\n\n# Convert the predictions into binary (0 or 1) using a threshold of 0.5\nensemble_preds_binary = threshold_predictions(ensemble_preds, threshold=0.5)\n\n# Calculate the ensemble's test accuracy\nensemble_accuracy = accuracy_score(y_test, ensemble_preds_binary)\nprint(f\"Ensemble Accuracy: {ensemble_accuracy:.4f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Export the results\n#data.to_csv('EURUSD_final.csv', index=False)\n#results.to_csv('final_results.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### ","metadata":{}}]}